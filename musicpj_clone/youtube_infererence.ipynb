{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"youtube_infererence.ipynb의 사본","provenance":[{"file_id":"1iVFGlRuhdpjtnO3a7ATzgd-lnPxtd4oT","timestamp":1629389283208}],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"ji80NTs-o34T"},"source":["# **META-LEARNING EXTRACTORS FOR MUSIC SOURCE SEPARATION**\n","\n","*David Samuel, Aditya Ganeshan & Jason Naradowsky*\n","\n"]},{"cell_type":"markdown","metadata":{"id":"K-5jiE6q1MJJ"},"source":["[**GitHub repository**](https://github.com/pfnet-research/meta-tasnet) | **[Paper](https://arxiv.org/abs/2002.07016)**\n","<br>\n","<br>"]},{"cell_type":"markdown","metadata":{"id":"Y-abx6lI1P5W"},"source":["We propose a hierarchical meta-learning-inspired model for music source separation in which **a generator model is used to predict the weights of individual extractor models**.  This enables efficient parameter-sharing, while still allowing for instrument-specific parameterization.  The resulting models are shown to be more effective than those trained independently or in a multi-task setting, and achieve performance comparable with state-of-the-art methods."]},{"cell_type":"markdown","metadata":{"id":"7bh1lQV-LG2a"},"source":["<p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/pfnet-research/meta-tasnet/master/img/parameter_generation.png\" alt=\"Overall architecture.\" width=\"512\"/>  \n","</p>\n","\n","<p align=\"center\">\n","<em>The overall architecture. The blue area depicts the parameter generator, a network which predicts the weights of the extractor's masking subnetwork specific to each instrument.  The extractor network then uses these weights when separating the instrument source from the mixture.</em>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"6kPUWF8T4LHF"},"source":["## Brief Introduction to Music Source Separation"]},{"cell_type":"markdown","metadata":{"id":"wop-WlV1xBH0"},"source":["Given a mixed source signal, the task of source separation algorithm is to divide the signal into its original components. We test our method on music separation and specifically on the [MUSDB18 dataset](https://zenodo.org/record/1117372#.XiSY9Bco9QJ) where the sources consist of contemporary songs and the goal is to divide them into four stems — **drums, vocals, bass and any other accompaniments**.\n","\n","Music source separation can not only be used as a preprocessing step to other MIR problems (like sound source identification), but it can also be used more creatively: we can create backing tracks to any song for musical practice or just for fun (karaoke), we can create \"smart\" equilizers that are able to make a new remix, or we can separate a single instrument to better study its intricacies (guitar players can more easily determine the exact chords for example). "]},{"cell_type":"markdown","metadata":{"id":"E1JC5DGn3wxr"},"source":["\n","\n","<p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/pfnet-research/meta-tasnet/master/img/spectrogram.png\" alt=\"Overall architecture.\" width=\"768\"/>  \n","</p>\n","\n","<p align=\"center\">\n","<em>Illustration of a separated audio signal (projected on log-scaled spectrograms). The top spectrogram shows the mixed audio that is transformed into the four separated components at the bottom. Note that we use the spectrograms just to illustrate the task — our model operates directly on the audio waveforms.</em>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"vU7Jgu7e4Vv9"},"source":["## Generating Extractor Models"]},{"cell_type":"markdown","metadata":{"id":"zp9Uge1r4jvz"},"source":["The key idea is to utilize a tiered architecture where a **generator** network \"supervises\" the training of the individual extractors by **generating some of their parameters directly**.  This allows the generator to develop a dense representation of how instruments relate to each other *as it pertains to the task*, and to utilize their commonalities when generating each extractor.\n","\n","Our model is based on [Conv-TasNet](https://arxiv.org/abs/1809.07454), a time domain-based approach to speech separation comprising three parts: \n","1. an **encoder** which applies a 1-D convolutional transform to a segment of the mixture waveform to produce a high-dimensional representation\n","2. a **masking function** which calculates a multiplicative function which identifies a targeted area in the learned representation \n","3. a **decoder** (1-D inverse convolutional layer) which reconstructs the separated waveform for the target source.\n","\n","The masking network is of particular interest, as it contains the source-specific masking information; the encoder and decoder are source-agnostic and remain fixed for separation of all sources.\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"36BT11Nt6lT3"},"source":["## Multi-stage Architecture"]},{"cell_type":"markdown","metadata":{"id":"D5aNcaNW66mV"},"source":["Despite the data's higher sampling rate (44kHz), we find that models trained using lower sampling rates are more effective despite the loss in resolution.  We therefore propose a multi-stage architecture to leverage this strength while still fundamentally predicting high resolution audio and use three stages with 8, 16 and 32kHz sampling rates"]},{"cell_type":"markdown","metadata":{"id":"wDtCu23Q7WJN"},"source":["<p align=\"center\">\n","  <img src=\"https://raw.githubusercontent.com/pfnet-research/meta-tasnet/master/img/multi_stage.png\" alt=\"Multi-stage architecture.\" width=\"512\"/> \n","</p>\n","\n","<p align=\"center\">\n","<em>Illustration of the multi-stage architecture. The resolution of the estimated signal is progressively enhanced by utilizing information from previous stages. The encoders increase the stride $s$ to preserve the same time dimension $T'$. Note that the masking TCN is still generated (not included in the illustration).</em>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"BiHQuk5M9K8H"},"source":["## Interactive Example\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1acunoJ0fdg0"},"source":["### 1. Initialize"]},{"cell_type":"code","metadata":{"id":"WPcaJd03acLe"},"source":["!pip install youtube-dl\n","!pip install soundfile\n","!git clone https://github.com/pfnet-research/meta-tasnet\n","\n","!wget \"https://www.dropbox.com/s/zw6zgt3edd88v87/best_model.pt\"\n","\n","import youtube_dl, soundfile, librosa, os, sys, torch, IPython.display\n","import numpy as np\n","from IPython.display import HTML\n","from google.colab import output, files\n","\n","sys.path.append(\"/content/meta-tasnet\")\n","from model.tasnet import MultiTasNet\n","\n","output.clear()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zScNng1Rff3Z"},"source":["### 2. Load the saved model"]},{"cell_type":"code","metadata":{"id":"KvGoD3JndIMl","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1629608915660,"user_tz":-540,"elapsed":1865,"user":{"displayName":"조범근","photoUrl":"","userId":"17573650045025658947"}},"outputId":"89ec89b6-9e4c-4969-c938-9423ce9f3ea3"},"source":["state = torch.load(\"best_model.pt\")  # load checkpoint\n","device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")  # optionally use the GPU\n","\n","network = MultiTasNet(state[\"args\"]).to(device)  # initialize the model\n","network.load_state_dict(state['state_dict'])  # load weights from the checkpoint"],"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<All keys matched successfully>"]},"metadata":{},"execution_count":2}]},{"cell_type":"markdown","metadata":{"id":"YgDm8ehsfkCK"},"source":["### 3. Define the separation procedure"]},{"cell_type":"code","metadata":{"id":"49WcEMxOdeOI"},"source":["# separate an audio clip (shape: [1, T]) with samping rate $rate\n","def separate_sample(audio, rate: int):\n","        \n","    def resample(audio, target_rate):\n","        return librosa.core.resample(audio, rate, target_rate, res_type='kaiser_best', fix=False)\n","    \n","    audio = audio.astype('float32')  # match the type with the type of the weights in the network\n","    mix = [resample(audio, s) for s in[8000, 16000, 32000]]  # resample to different sampling rates for the three stages\n","    mix = [librosa.util.fix_length(m, (mix[0].shape[-1]+1)*(2**i)) for i,m in enumerate(mix)]  # allign all three sample so that their lenghts are divisible\n","    mix = [torch.from_numpy(s).float().to(device).view(1, 1, -1) for s in mix]  # cast to tensor with shape: [1, 1, T']\n","    mix = [s / s.std(dim=-1, keepdim=True) for s in mix]  # normalize by the standard deviation\n","    \n","    network.eval()\n","    with torch.no_grad():        \n","        separation = network.inference(mix, n_chunks=2)[-1]  # call the network to obtain the separated audio with shape [1, 4, 1, T']\n","\n","    # normalize the amplitudes by computing the least squares\n","    # -> we try to scale the separated stems so that their sum is equal to the input mix \n","    a = separation[0,:,0,:].cpu().numpy().T  # separated stems\n","    b = mix[-1][0,0,:].cpu().numpy()  # input mix\n","    sol = np.linalg.lstsq(a, b, rcond=None)[0]  # scaling coefficients that minimize the MSE\n","    separation = a * sol  # scale the separated stems\n","\n","    estimates = {\n","        'drums': separation[:,0:1],\n","        'bass': separation[:,1:2],\n","        'other': separation[:,2:3],\n","        'vocals': separation[:,3:4],\n","    }\n","\n","    return estimates"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MIfS3Qlmfo2B"},"source":["### 4. Load a song from youtube\n","\n","Choose a song to separate within a time interval and hit play to load that song from youtube.\n","\n","Note that you can override the variable `id` and separate whatever song you want (use the \"id\" string from the URL address on youtube)! Just keep in mind that the audio should be of high quality (which isn't always the case on youtube, unfortunately)."]},{"cell_type":"code","metadata":{"id":"vgL36Jjjdv_J","colab":{"base_uri":"https://localhost:8080/","height":91,"output_embedded_package_id":"1FYfQzeysQuA1EuBppJiEDL2Ok7WW6N0b"},"executionInfo":{"status":"ok","timestamp":1629608971286,"user_tz":-540,"elapsed":10814,"user":{"displayName":"조범근","photoUrl":"","userId":"17573650045025658947"}},"outputId":"9b18eee4-c0cc-4f8f-bbff-0d9dcf68d98d"},"source":["ids = {\n","    \"비밀의 화원\": \"U6FopXugJo8\",\n","    \"Marvin Gaye - Whats Happening Brother (soul)\": \"HO5UH4uX_yA\",\n","    \"Dire Straits - Sultans Of Swing (rock)\": \"0fAQhSRLQnM\", \n","    \"Billie Eilish - Bad Guy (pop)\": \"DyDfgMOUjCI\",\n","    \"Death - Spirit Crusher (death metal)\": \"4_rYk_aJbcQ\",\n","    \"James Brown - Get Up (I Feel Like Being A) Sex Machine (funk)\": \"kwjHpi4rXb8\",\n","    \"Věra Bílá & Kale - Pas o panori (world music)\": \"R-L477kx8LA\",\n","    \"Eminem - Lose Yourself (hip-hop)\": \"nPA2czkOsFE\",\n","    \"Sting - Englishman in New York (pop/rock)\": \"d27gTrPPAyk\",\n","    \"R.E.M. - Losing my Religion (alternative rock)\": \"xwtdhWltSIg\",\n","    \"AURORA – Animal (pop)\": \"3DIT8Y3LC6M\",\n","    \"Red Hot Chili Peppers - Scar Tissue (alternative rock)\": \"mzJj5-lubeM\",\n","    \"John Mayer - Gravity (blues)\": \"7VBex8zbDRs\",\n","    \"Darude - Sandstorm (EDM)\": \"y6120QOlsfU\",\n","    \"Pokemon (soundtrack)\": \"JuYeHPFR3f0\",\n","    \"Daft Punk - Get Lucky (pop)\": \"5NV6Rdv1a3I\",\n","    \"Maroon 5 feat. Christina Aguilera - Moves Like Jagger (pop)\": \"suRsxpoAc5w\"\n","}\n","\n","song = \"비밀의 화원\" #@param [\"비밀의 화원\", \"Marvin Gaye - Whats Happening Brother (soul)\", \"Red Hot Chili Peppers - Scar Tissue (alternative rock)\", \"Daft Punk - Get Lucky (pop)\", 'Billie Eilish - Bad Guy (pop)','Death - Spirit Crusher (death metal)', \"James Brown - Get Up (I Feel Like Being A) Sex Machine (funk)\", 'Věra Bílá & Kale - Pas o panori (world music)','Eminem - Lose Yourself (hip-hop)','Sting - Englishman in New York (pop/rock)','R.E.M. - Losing my Religion (alternative rock)', \"AURORA – Animal (pop)\", 'Dire Straits - Sultans Of Swing (rock)', \"John Mayer - Gravity (blues)\", \"Darude - Sandstorm (EDM)\", \"Pokemon (soundtrack)\", \"Maroon 5 feat. Christina Aguilera - Moves Like Jagger (pop)\"]\n","start = 41 #@param {type:\"slider\", min:0, max:180, step:1}\n","stop = 73 #@param {type:\"slider\", min:0, max:180, step:1}\n","\n","id = ids[song]  # change this for you own song\n","\n","ydl_opts = {\n","    'format': 'bestaudio/best', \n","    'postprocessors': [{'key': 'FFmpegExtractAudio','preferredcodec': 'wav','preferredquality': '44100',}],\n","    'outtmpl': 'tmp.wav'\n","}\n","with youtube_dl.YoutubeDL(ydl_opts) as ydl:\n","    status = ydl.download([id])\n","\n","audio, rate = librosa.load('tmp.wav', sr=None)\n","os.remove('tmp.wav')\n","\n","start_pad, stop_pad = max(0, start-4), min(audio.shape[-1]/rate-1, stop+4)\n","start_cut, stop_cut = start-start_pad, stop-stop_pad\n","\n","audio = audio[start_pad*rate:stop_pad*rate].copy()\n","\n","output.clear()\n","print(f\"{song}\")\n","IPython.display.display(IPython.display.Audio(audio[start_cut*rate:stop_cut*rate].copy(), rate=rate))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"gVnKT9I8gJbB"},"source":["### 5. Separate!"]},{"cell_type":"code","metadata":{"id":"kcROr5atXtNE","colab":{"base_uri":"https://localhost:8080/","height":472,"output_embedded_package_id":"1QbV66IbqKIrp7TFrpgVA-wsOp3ageZb8"},"executionInfo":{"status":"ok","timestamp":1629609029888,"user_tz":-540,"elapsed":17925,"user":{"displayName":"조범근","photoUrl":"","userId":"17573650045025658947"}},"outputId":"31c054a1-4baa-4e09-8235-45e7d4fee952"},"source":["print(\"separating... \", end='')\n","estimates = separate_sample(audio, rate)\n","estimates = {i: e[start_cut*32000:stop_cut*32000,:] for i, e in estimates.items()}  # cut to show only the desired part (mainly to reduce the latency)\n","print(\"done\")\n","print(\"downloading audio files to the client side...\")\n","\n","for instrument in ['vocals', 'drums', 'bass', 'other']:\n","    if estimates[instrument].max() < 0.25: continue  # hacky way to remove the silent instruments\n","\n","    print(f\"\\n{instrument}\")\n","    IPython.display.display(IPython.display.Audio(estimates[instrument].T.copy(), rate=32000))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"eptItqiaJw5O"},"source":["### 6. Create a backing track for karaoke"]},{"cell_type":"code","metadata":{"id":"intWQwBrKYZy","colab":{"base_uri":"https://localhost:8080/","height":73,"output_embedded_package_id":"1hY9AO4Htjj4W2CL5zcCMeC396AcXdlsT"},"executionInfo":{"status":"ok","timestamp":1629609055249,"user_tz":-540,"elapsed":3028,"user":{"displayName":"조범근","photoUrl":"","userId":"17573650045025658947"}},"outputId":"14fb529c-bc1f-4fb2-e802-0e6510689805"},"source":["mix = estimates[\"drums\"] + estimates[\"bass\"] + estimates[\"other\"]\n","IPython.display.display(IPython.display.Audio(mix.T.copy(), rate=32000))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"E1NkdzZEe05L"},"source":["### 7. Remix\n","\n","Let's say would like to make the awesome bassline more pronounced... Well, why not remix the song to our taste?\n","\n","Set the volume of different instruments and hit play to compute a new mix.\n"]},{"cell_type":"code","metadata":{"id":"uN9lY4WdamQp","cellView":"form","colab":{"base_uri":"https://localhost:8080/","height":73,"output_embedded_package_id":"1AW_3AOMqTKYHyDsvRKAxn02ibEV4Bll9"},"executionInfo":{"status":"ok","timestamp":1629609061022,"user_tz":-540,"elapsed":2874,"user":{"displayName":"조범근","photoUrl":"","userId":"17573650045025658947"}},"outputId":"eeeca0c0-3730-4f64-d708-cd7540f479cd"},"source":["vocals = 1.2 #@param {type:\"slider\", min:0, max:2, step:0.1}\n","drums = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n","bass = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n","other = 0.3 #@param {type:\"slider\", min:0, max:2, step:0.1}\n","\n","mix = estimates[\"vocals\"]*vocals + \\\n","      estimates[\"drums\"]*drums + \\\n","      estimates[\"bass\"]*bass + \\\n","      estimates[\"other\"]*other\n","\n","IPython.display.display(IPython.display.Audio(mix.T.copy(), rate=32000))"],"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"Output hidden; open in https://colab.research.google.com to view."},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"_0ffSPSJKmHD"},"source":[""],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NwB_6vEwH3Bg"},"source":["### 8. Load from an uncompressed file\n","\n","Unfortunately, the youtube videos are compressed, so the sepaparation quality is not as good as it could be. Load an uncompressed file from your computer (click on `Upload` in the left toolbar) to obtain better results: "]},{"cell_type":"code","metadata":{"id":"Sxl7WchjJKCm","colab":{"base_uri":"https://localhost:8080/","height":385},"executionInfo":{"status":"error","timestamp":1629609089900,"user_tz":-540,"elapsed":312,"user":{"displayName":"조범근","photoUrl":"","userId":"17573650045025658947"}},"outputId":"e6089d4b-0fbc-451a-cc2b-ff46a7461cf2"},"source":["filename = \"scar_tissue.wav\" #@param {type:\"string\"}\n","start = 40 #@param {type:\"slider\", min:0, max:180, step:1}\n","stop = 50 #@param {type:\"slider\", min:0, max:180, step:1}\n","\n","audio, rate = soundfile.read(filename)\n","audio = librosa.core.to_mono(audio.transpose())\n","\n","print(audio.shape, rate)\n","start_pad, stop_pad = max(0, start-4), min(audio.shape[-1]/rate-1, stop+4)\n","start_cut, stop_cut = start-start_pad, stop-stop_pad\n","\n","audio = audio[start_pad*rate:stop_pad*rate].copy()\n","audio = np.expand_dims(audio, 0)\n","\n","output.clear()\n","print(f\"{filename} mix:\")\n","IPython.display.display(IPython.display.Audio(audio[:, start_cut*rate:stop_cut*rate].copy(), rate=rate))\n","\n","print()\n","print(\"separating... \", end='')\n","estimates = separate_sample(audio, rate)\n","print(\"done\")\n","print(\"downloading audio files to the client side...\")\n","\n","for instrument in ['vocals', 'drums', 'bass', 'other']:\n","    separation = estimates[instrument][start_cut*32000:stop_cut*32000,:]  # cut to show only the desired part (mainly to reduce the latency)\n","    if separation.max() < 0.25: continue  # hacky way to remove the silent instruments\n","\n","    print(f\"\\n{instrument}\")\n","    IPython.display.display(IPython.display.Audio(separation.T.copy(), rate=32000))"],"execution_count":null,"outputs":[{"output_type":"error","ename":"RuntimeError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)","\u001b[0;32m<ipython-input-8-0c8dbbb7bb1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mstop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m50\u001b[0m \u001b[0;31m#@param {type:\"slider\", min:0, max:180, step:1}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0maudio\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msoundfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0maudio\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_mono\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maudio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36mread\u001b[0;34m(file, frames, start, stop, dtype, always_2d, fill_value, out, samplerate, channels, format, subtype, endian, closefd)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \"\"\"\n\u001b[1;32m    256\u001b[0m     with SoundFile(file, 'r', samplerate, channels,\n\u001b[0;32m--> 257\u001b[0;31m                    subtype, endian, format, closefd) as f:\n\u001b[0m\u001b[1;32m    258\u001b[0m         \u001b[0mframes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_prepare_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mframes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    259\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mframes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malways_2d\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, samplerate, channels, subtype, endian, format, closefd)\u001b[0m\n\u001b[1;32m    627\u001b[0m         self._info = _create_info_struct(file, mode, samplerate, channels,\n\u001b[1;32m    628\u001b[0m                                          format, subtype, endian)\n\u001b[0;32m--> 629\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode_int\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosefd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    630\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0missuperset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'r+'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseekable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    631\u001b[0m             \u001b[0;31m# Move write position to 0 (like in Python file objects)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_open\u001b[0;34m(self, file, mode_int, closefd)\u001b[0m\n\u001b[1;32m   1182\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid file: {0!r}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1183\u001b[0m         _error_check(_snd.sf_error(file_ptr),\n\u001b[0;32m-> 1184\u001b[0;31m                      \"Error opening {0!r}: \".format(self.name))\n\u001b[0m\u001b[1;32m   1185\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mmode_int\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSFM_WRITE\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1186\u001b[0m             \u001b[0;31m# Due to a bug in libsndfile version <= 1.0.25, frames != 0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.7/dist-packages/soundfile.py\u001b[0m in \u001b[0;36m_error_check\u001b[0;34m(err, prefix)\u001b[0m\n\u001b[1;32m   1355\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0merr\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1356\u001b[0m         \u001b[0merr_str\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_snd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msf_error_number\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1357\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprefix\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0m_ffi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_str\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'replace'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1359\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mRuntimeError\u001b[0m: Error opening 'scar_tissue.wav': System error."]}]}]}